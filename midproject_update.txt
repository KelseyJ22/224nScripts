We have implemented skeleton models to classify 5-grams to decades and centuries in various different architectures: RNN with RNN and GRU cells and window-based with two-hidden-layer feedforward or one-hidden-layer softmax classifier. Currently, we have parsed our data into approximately 1GB files which we then separate into 80% train, 10% test, 10% dev. We have also trained on a balanced dataset with the same number of examples from each label (century or decade) to solve the problem caused by the quantity of 20th century documents relative to all other documents, which significantly improved performance but restricted our dataset (and meant we didn't include decades/centuries that occur infrequently at all in any of the dev/test/train datasets). Next, we will implement hard negative mining so we can take advantage of all of our data rather than selecting only some of it as we will have to do with the balanced dataset. We have run each of our models on the GPU with the 1GB dataset. It was still quite slow (about a full day for all of them) and we plan to experiment with the batch size to hopefully improve performance. After this, we will select one of these models to keep working with and tune hyperparameters and make other modifications from there.